<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="This is the webpage of Trung X. Pham.">
  <meta name="keywords" content="PXT, Trung X. Pham, Pham Xuan Trung, KAIST">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Pham Xuan Trung</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--   <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
 -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/Scholar_TrungPham.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://trungpx.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
<!--           <a class="navbar-item" href="https://scholar.google.com/citations?user=4DkPIIAAAAAJ&hl=en"> -->
          <a class="navbar-item" "https://scholar.google.com/citations?user=4DkPIIAAAAAJ&hl=en">
            Audio-guided Videos Synthesis
          </a>
<!--           <a class="navbar-item" href="http://sanctusfactory.com/u-aim/"> -->
          <a class="navbar-item" "http://sanctusfactory.com/u-aim/">
            Virtual Try-On Generation
          </a>
<!--           <a class="navbar-item" href="https://scholar.google.com/citations?user=4DkPIIAAAAAJ&hl=en"> -->
          <a class="navbar-item" "https://scholar.google.com/citations?user=4DkPIIAAAAAJ&hl=en">
            Representation Learning
          </a>
<!--           <a class="navbar-item" href="https://scholar.google.com/citations?user=4DkPIIAAAAAJ&hl=en"> -->
          <a class="navbar-item" "https://scholar.google.com/citations?user=4DkPIIAAAAAJ&hl=en">
            Multimodal Generation
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Pham Xuan Trung</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b>Korea Advanced Institute of Science and Technology (KAIST)</b></span>
          </div>


      <table style="width:100%;max-width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:0%;width:75%;vertical-align:middle">
                <p class="name" style="text-align: justify;color:blue;">
                  <b>Personal Information</b>
                </p>
                <p style="text-align: justify">
                  Greetings, I am pursuing my <i>integrated Ms-Ph.D.</i> degree in Electrical Engineering at <b>KAIST</b> in 2018-2025, 
                  under the guidance of Professor <a href="http://sanctusfactory.com/u-aim/"> <b>Chang D. Yoo</b></a>. My interested topics relate to computer vision, deep learning, generative AI, 
                  and image/video/audio processing: <i>Self-supervised Learning, Generative Models, Diffusion Models, Multimodal Learning, 
                  Speech Processing, and Natural Language Processing.</i>
                </p>
                <p style="text-align: justify">
                  In the past, I graduated in 2014 from Hanoi University of Science and Technology (HUST, a top-tier university in Viet Nam), with the School of Electronics and Telecommunications ranked 10/526 students (<b>top 1.9%</b>).
                  After that, I worked for VNPT Technology Corporation (employees > 1000+) in Hanoi for 3 years until 2018, mainly doing research and deploying 2G, 3G & 4G mobile communication network projects with various 
                  vendors such as Alcatel-Lucent, Nokia Siemens, and SAMSUNG.
                </p>
                <p style="text-align: center">
                  <a href="mailto:phamxuantrungbk@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="./static/trungpx_cv.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=4DkPIIAAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/phamxuantrungbk/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:0%;width:50%;max-width:50%;vertical-align:right">
                <a href="https://trungpx.github.io/"><img style="width:75%;max-width:75%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="./static/images/Trung.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <div class="content has-text-justified">
          <p style="color:blue;"><b>International Conferences and Journals</b></p>
          <p>
            I take immense pride in my contributions to the academic community, having disseminated my findings through publications in top-tier venues:
            <b>NeurIPS (1), ICML (1), ICLR (2), CVPR (3), ECCV (1), ICASSP (1), Advanced Materials (1, IF: 32), Nano Energy (1, IF: 19), IEEE TCSTV (1, IF: 8.4), IEEE TBD (1, IF: 4.5), IEEE Access (3, IF: 3.9)</b>. 
          </p>
          <p style="color:blue;"><b>Reviewers & Program Committee Members</b></p>
          <p>
            I served as a Reviewer and Program Committee at various prestigious conferences and journals
            <ol>
              <li><b>CVPR 2023, 2024, 2025</b> [The Conference on Computer Vision and Pattern Recognition]</li>
              <li><b>NeurIPS 2023, 2024</b> [The Conference on Neural Information Processing Systems]</li>
              <li><b>ICML 2024, ICML 2025</b> [The International Conference on Machine Learning]</li>
              <li><b>AAAI 2024, AAAI 2025</b> [The Association for the Advancement of Artificial Intelligence]</li>
              <li><b>ICCV 2023</b> [The International Conference on Computer Vision]</li>
              <li><b>ECCV 2024</b> [The European Conference on Computer Vision]</li>
              <li><b>ICLR 2024, ICLR 2025</b> [The International Conference on Learning Representations]</li>
              <li><b>ACCV 2024</b> [The Asian Conference on Computer Vision]</li>
              <li><b>ICASSP 2024, ICASSP 2025</b> [The International Conference on Acoustics, Speech, and Signal Processing]</li>
              <li><b>AISTATS 2025</b> [International Conference on Artificial Intelligence and Statistics]</li>
              <li><b>IJCNN 2025</b> [International Joint Conference on Neural Networks]</li>
              <li>Neural Networks <b>(NN) 2023, Impact Factor: 8.67</b></li>
              <li>IEEE Transaction on Multimedia <b>(TMM) 2023, Impact Factor: 7.39</b></li>
              <li>Computer Vision and Image Understanding <b>(CVIU) 2024, Impact Factor: 4.3</b></li>
              <li>ISPRS Journal of Photogrammetry and Remote Sensing <b>(ISPRS) 2024, Impact Factor: 11.83</b></li>
              <li>Expert Systems With Applications <b>(ESWA) 2025, Impact Factor: 8.5</b></li>
              <li>Digital Signal Processing <b>(DSP) 2025, Impact Factor: 3.4</b></li>
            </ol>
          </p>
            
          <p style="color:blue;"><b>Awards</b></p>
          <p>
            <ul>
              <li>Won First Prize in Mathematic Contest for High School Student Grade 12, 2009 </li>
              <li>Won Trang Nguyen Flower award for the best student among thousands of students in Giao Thuy B High School, 2009 </li>
              <li>Annual Encourage Scholarship by Hanoi University of Science and Technology (HUST) for excellent students with outstanding performance for every semester, 2009 â€“ 2014 </li>
              <li>Won Award of the Top 100 Best Korea National Researches, 2023 <a href="./static/certificate_casebook_2023_top_100_research.pdf">[Certificate]</a> </li>
            </ul>
          </p>
          </div>
        <section class="section">
          <div class="container is-max-desktop">
            <!-- Intro -->
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <h2 class="title is-3">Recent Publications</h2>
                <p> * denotes equal contributions, my research was first recorded in 2018. </p>
              </div>
            </div>
          </div>
        </section>




        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/ERAM.png"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://arxiv.org/abs/xxxx.xxxxx"> [2025] E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot Object Customization </a>
                </p>
                <p class="name" style="text-align: justify;">
                <b>Trung X. Pham</b>, Zhang Kang, Hong Ji Woo, Xuran Zheng, Chang D. Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em>Arxiv Feb 2025</em> 
                  <a href="https://arxiv.org/abs/xxxx.xxxxx">[OpenReview]</a>
                  <a href="https://github.com/trungpx/emd3c">[Code]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                A state-of-the-art framework, a novel framework for vision-inspired generation optimized for model parameter size, memory consumption, and inference speed using denoising masked diffusion transformers, facilitating efficient zero-shot object customization without reliance on giant pre-trained diffusion models as existing works.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/MDSGen.png"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://arxiv.org/abs/2410.02130">[2025] MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers for Open-Domain Sound Generation</a>
                </p>
                <p class="name" style="text-align: justify;">
                <b>Trung X. Pham*</b>, Tri Ton*, and Chang D. Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em>International Conference on Learning Representations (ICLR 2025), held in  Singapore</em> 
                  <a href="https://openreview.net/forum?id=yFEqYwgttJ">[OpenReview]</a>
                  <a href="https://github.com/trungpx/mdsgen">[Code]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                A state-of-the-art framework, a novel framework for vision-guided open-domain sound generation optimized for model parameter size, memory consumption, and inference speed using denoising masked diffusion transformers, facilitating efficient generation without reliance on pre-trained diffusion models.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/xmdpt.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://arxiv.org/abs/2402.01516">[2024] Cross-view Masked Diffusion Transformers for Person Image Synthesis</a>
                </p>
                <p class="name" style="text-align: justify;">
                <b>Trung X. Pham*</b>, Zhang Kang*, and Chang D. Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em>International Conference on Machine Learning (ICML 2024), held in Vienna, Austria</em> 
                  <a href="https://openreview.net/pdf?id=jEoIkNkqyc">[OpenReview]</a>
                  <a href="https://github.com/trungpx/xmdpt">[Code]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                A state-of-the-art framework for pose-guided human image synthesis using the cutting-edge technique of masked diffusion transformers.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/acdmsr.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://ieeexplore.ieee.org/abstract/document/10477506/">[2024] ACDMSR: Accelerated conditional diffusion models for single image super-resolution</a>
                </p>
                <p class="name" style="text-align: justify;">
                Axi Niu, <b>Trung X. Pham</b>, Kang Zhang, Jinqiu Sun, Yu Zhu, Qingsen Yan, In So Kweon, Yanning Zhang
                </p>
                <p class="name" style="text-align: justify;">
                <em>IEEE Transactions on Broadcasting (TBD 2024), IF: 5.19</em> 
                  <a href="https://ieeexplore.ieee.org/abstract/document/10477506/">[Links IEEE]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                A novel framework for Speeding up Image Super-Resolution.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/mpfnet.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://ieeexplore.ieee.org/abstract/document/10463059/">[2024] Learning from multi-perception features for real-word image super-resolution</a>
                </p>
                <p class="name" style="text-align: justify;">
                Axi Niu, Kang Zhang, <b>Trung X. Pham</b>, Pei Wang, Jinqiu Sun, In So Kweon, Yanning Zhang
                </p>
                <p class="name" style="text-align: justify;">
                <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT 2024), IF: 8.4</em> 
                  <a href="https://ieeexplore.ieee.org/abstract/document/10463059/">[Links IEEE]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                MPF-Net for Single Image Super-Resolution.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/resmoco.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://ieeexplore.ieee.org/abstract/document/10287941/">[2023] Self-supervised visual representation learning via residual momentum</a>
                </p>
                <p class="name" style="text-align: justify;">
                <b>Trung X. Pham</b>, Axi Niu, Kang Zhang, Tee Joshua Tian Jin, Ji Woo Hong, Chang D Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em>IEEE Access 2023, acceptance rate 30%</em> 
                  <a href="https://ieeexplore.ieee.org/abstract/document/10287941/">[Links IEEE]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                Introduction of residual momentum that significantly improves self-supervised learning frameworks.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/dimcl.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://ieeexplore.ieee.org/abstract/document/10014996/">[2023] DimCL: Dimensional Contrastive Learning for Improving Self-Supervised Learning</a>
                </p>
                <p class="name" style="text-align: justify;">
                Thanh Nguyen*, <b>Trung X. Pham*</b>, Chaoning Zhang, Tung M Luu, Thang Vu, Chang D Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em>IEEE Access 2023, acceptance rate 30%</em> 
                  <a href="https://ieeexplore.ieee.org/abstract/document/10014996/">[Links IEEE]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                Introduction of a new regularization that significantly improves self-supervised learning frameworks.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/cdpmsr.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://ieeexplore.ieee.org/abstract/document/10222191/">[2023] Cdpmsr: Conditional diffusion probabilistic models for single image super-resolution</a>
                </p>
                <p class="name" style="text-align: justify;">
                Axi Niu, Kang Zhang, <b>Trung X. Pham</b>, Jinqiu Sun, Yu Zhu, In So Kweon, Yanning Zhang
                </p>
                <p class="name" style="text-align: justify;">
                <em> IEEE International Conference on Image Processing (ICIP) 2023, acceptance rate 47%</em> 
                  <a href="https://arxiv.org/pdf/2302.12831">[Links]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                Conditional diffusion model for image super-resolution with post-process technique
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/nanoenergy.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://www.sciencedirect.com/science/article/pii/S2211285522006887">[2022] Deep learning-based noise robust flexible piezoelectric acoustic sensors for speech processing</a>
                </p>
                <p class="name" style="text-align: justify;">
                Young Hoon Jung*, <b>Trung X. Pham*</b>, Dias Issa, Hee Seung Wang, Jae Hee Lee, Mingi Chung, Bo-Yeon Lee, Gwangsu Kim, Chang D Yoo, Keon Jae Lee
                </p>
                <p class="name" style="text-align: justify;">
                <em> Nano Energy (IF: 19.0) 2022</em> 
                  <a href="https://www.sciencedirect.com/science/article/pii/S2211285522006887">[Links]</a>
                </p>
                <p class="name" style="text-align:justify;color:red;"> >> Top 100 best Korea national researches 2023 <a href="./static/certificate_casebook_2023_top_100_research.pdf">[Certificate]</a></p>
                <br>
                <p class="name" style="text-align: justify;">
                An excellent combination of deep learning and flexible piezoelectric acoustic sensor for >99% accuracy of speaker recognition.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/howdoes.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://openreview.net/forum?id=bwq6O4Cwdl">[2022] How does simsiam avoid collapse without negative samples? a unified understanding with self-supervised contrastive learning</a>
                </p>
                <p class="name" style="text-align: justify;">
                Chaoning Zhang, Kang Zhang, Chenshuang Zhang, <b>Trung X. Pham</b>, Chang D Yoo, In So Kweon
                </p>
                <p class="name" style="text-align: justify;">
                <em> International Conference on Learning Representations (ICLR 2022), acceptance rate 32.9%</em> 
                  <a href="https://openreview.net/forum?id=bwq6O4Cwdl">[OpenReview]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                A deep analysis of constrastive learning frameworks to clarify the collapse issue.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/dualtemp.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Dual_Temperature_Helps_Contrastive_Learning_Without_Many_Negative_Samples_Towards_CVPR_2022_paper.html">[2022] Dual temperature helps contrastive learning without many negative samples: Towards understanding and simplifying moco</a>
                </p>
                <p class="name" style="text-align: justify;">
                Chaoning Zhang*, Kang Zhang*, <b>Trung X. Pham*</b>, Axi Niu, Zhinan Qiao, Chang D Yoo, In So Kweon
                </p>
                <p class="name" style="text-align: justify;">
                <em> The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022), acceptance rate 25.3%</em> 
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Dual_Temperature_Helps_Contrastive_Learning_Without_Many_Negative_Samples_Towards_CVPR_2022_paper.pdf">[Links]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                The new loss function is proposed to reduce the massive number of negative samples in contrastive learning frameworks.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/onpros.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://arxiv.org/abs/2208.05744">[2022] On the pros and cons of momentum encoder in self-supervised visual representation learning</a>
                </p>
                <p class="name" style="text-align: justify;">
                <b>Trung X. Pham</b>, Chaoning Zhang, Axi Niu, Kang Zhang, Chang D Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em> Arxiv 2022</em> 
                  <a href="https://arxiv.org/abs/2208.05744">[Links]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                A deep investigation on the pros and cons of EMA-based contrastive learning frameworks.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/lad.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://ieeexplore.ieee.org/abstract/document/9924165/">[2022] Lad: A hybrid deep learning system for benign paroxysmal positional vertigo disorders diagnostic</a>
                </p>
                <p class="name" style="text-align: justify;">
                <b>Trung X. Pham*</b>, Jin Woong Choi*, Rusty John Lloyd Mina, Thanh Xuan Nguyen, Sultan Rizky Madjid, Chang D Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em> IEEE Access 2022, acceptance rate 30%</em> 
                  <a href="https://ieeexplore.ieee.org/abstract/document/9924165/">[Links IEEE]</a>
                  <a href="https://github.com/trungpx/lad">[Code]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                Using AI deep learning to diagnose BPPV disorders in patients in hospital, data from Chungnam National Hospital University.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/rotnet.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://arxiv.org/abs/2108.00475">[2021] Self-supervised Learning with Local Attention-Aware Feature</a>
                </p>
                <p class="name" style="text-align: justify;">
                <b>Trung X. Pham*</b>, Rusty John Lloyd Mina, Dias Issa, Chang D. Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em> Arxiv 2021</em> 
                  <a href="https://arxiv.org/abs/2108.00475">[Links]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                Learning representation of the data without any labels.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/maml.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://ieeexplore.ieee.org/abstract/document/9413446/">[2021] Robust MAML: Prioritization task buffer with adaptive learning process for model-agnostic meta-learning</a>
                </p>
                <p class="name" style="text-align: justify;">
                Thanh Nguyen, Tung Luu, <b>Trung X. Pham</b>, Sanzhar Rakhimkul, Chang D Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em> IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2021, acceptance rate 48.4%</em> 
                  <a href="https://ieeexplore.ieee.org/abstract/document/9413446/">[Links IEEE]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                Learning meta model with adaptive learning rate scheme.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/influence.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Lee_Learning_Augmentation_Network_via_Influence_Functions_CVPR_2020_paper.html">[2020] Learning augmentation network via influence functions</a>
                </p>
                <p class="name" style="text-align: justify;">
                Donghoon Lee, Hyunsin Park, <b>Trung X. Pham</b>, Chang D Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020), acceptance rate 22%</em> 
                  <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Lee_Learning_Augmentation_Network_via_Influence_Functions_CVPR_2020_paper.html">[Links IEEE]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                Learning augmentation with the differentiable neural network with a fancy influence approach.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/vqa.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Kim_Modality_Shifting_Attention_Network_for_Multi-Modal_Video_Question_Answering_CVPR_2020_paper.html">[2020] Modality shifting attention network for multi-modal video question answering</a>
                </p>
                <p class="name" style="text-align: justify;">
                Junyeong Kim, Minuk Ma, <b>Trung X. Pham</b>, Kyungsu Kim, Chang D Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020), acceptance rate 22%</em> 
                  <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Kim_Modality_Shifting_Attention_Network_for_Multi-Modal_Video_Question_Answering_CVPR_2020_paper.html">[Links IEEE]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                Video question answering via a smart design neural network.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/cascaderpn.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://proceedings.neurips.cc/paper_files/paper/2019/hash/d554f7bb7be44a7267068a7df88ddd20-Abstract.html">[2019] Cascade rpn: Delving into high-quality region proposal network with adaptive convolution</a>
                </p>
                <p class="name" style="text-align: justify;">
                Thang Vu, Hyunjun Jang, <b>Trung X. Pham</b>, Chang D Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em> Advances in Neural Information Processing Systems (NeurIPS 2019), acceptance rate 21.6%</em> 
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2019/hash/d554f7bb7be44a7267068a7df88ddd20-Abstract.html">[Links]</a>
                </p>
                <br>
                <p class="name" style="text-align:justify;color:red;"> >> Spotlight (top 2.4%)</p>
                <p class="name" style="text-align: justify;">
                Significantly improve object detection of 2D images with a ground-breaking design.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/advancedmaterials.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/adma.201904020">[2020] Flexible piezoelectric acoustic sensors and machine learning for speech processing</a>
                </p>
                <p class="name" style="text-align: justify;">
                Young Hoon Jung, Seong Kwang Hong, Hee Seung Wang, Jae Hyun Han, <b>Trung X. Pham</b>, Hyunsin Park, Junyeong Kim, Sunghun Kang, Chang D Yoo, Keon Jae Lee
                </p>
                <p class="name" style="text-align: justify;">
                <em> Advanced Materials 2020 (IF: 32)</em> 
                  <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/adma.201904020">[Links]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                Combining AI/machine learning with flexible acoustic sensors for Speech Processing.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/feqe.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://openaccess.thecvf.com/content_eccv_2018_workshops/w25/html/Vu_Fast_and_Efficient_Image_Quality_Enhancement_via_Desubpixel_Convolutional_Neural_ECCVW_2018_paper.html">[2018] Fast and efficient image quality enhancement via desubpixel convolutional neural networks</a>
                </p>
                <p class="name" style="text-align: justify;">
                Thang Vu, Cao Van Nguyen, <b>Trung X. Pham</b>, Tung M Luu, Chang D Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em> Proceedings of the European Conference on Computer Vision (ECCV 2018), acceptance rate 31.8% </em> 
                  <a href="https://openaccess.thecvf.com/content_eccv_2018_workshops/w25/html/Vu_Fast_and_Efficient_Image_Quality_Enhancement_via_Desubpixel_Convolutional_Neural_ECCVW_2018_paper.html">[Links]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                Efficient framework for image super-resolution
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle"><img src="./static/images/cnnspeaker.jpg"></td>
              <td width="70%" valign="center">
                <p class="name" style="text-align: justify;">
                <a href="https://www.researchgate.net/publication/336086517_Short_Convolutional_Neural_Network_and_MFCCs_for_Accurate_Speaker_Recognition_Systems">[2019] Short Convolutional Neural Network and MFCCs for Accurate Speaker Recognition Systems</a>
                </p>
                <p class="name" style="text-align: justify;">
                <b>Trung X. Pham</b> and Chang D Yoo
                </p>
                <p class="name" style="text-align: justify;">
                <em> The 34th International Technical Conference on Circuits/Systems, Computers and Communications (ITC-CSCC 2019) </em> 
                  <a href="https://www.researchgate.net/publication/336086517_Short_Convolutional_Neural_Network_and_MFCCs_for_Accurate_Speaker_Recognition_Systems">[Links]</a>
                </p>
                <br>
                <p class="name" style="text-align: justify;">
                A lightweight, accurate, and efficient deep neural network for speaker recognition systems.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>


          


          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2402.01516"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.01516"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/trungpx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">

<!--   <div class="container is-max-desktop"> -->
    <div class="columns is-centered">
    <!-- Co-works. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Co-operations</h2>
        <div class="content has-text-justified">
          <p>
            I am open to collaborating with researchers on various topics in Deep Learning, Machine Learning, and AI, including but not limited to Computer Vision, Audio Processing, and Natural Language Processing (NLP).
            Feel free to contact me at: trungpx@kaist.ac.kr or phamxuantrungbk@gmail.com
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->
  </div>
</section>

<footer class="footer">
  <div class="container">
<!--     <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/trungpx" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website used code from <a href="https://github.com/nerfies">this</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
